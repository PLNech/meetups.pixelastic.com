---
layout: post
title: "dotScale 2016"
tags: dotscale
---

After dotSecurity last Friday, it is now dotScale. This follow the same pattern
as the dotCSS/dotJS conferences. One short (one afternoon) on the Friday and one
longer (one full day) on the next Monday.

As usual, this took place in the Théatre de Paris. The main issue of the last
dotJS that took place here (the main room getting hotter and hotter along the
day) was fixed, which let us completely enjoy the talks. There were more than
800 attendees this time.

# Mickael Rémond

First talk was by Mickael Rémond, about the Erlang language. It was a high level
talk, explaining the history of the language. 

It started in 1973, in big telecommunication firms. It follows what is called
the Actor Model. Each actor has a single responsability and can send messages to
other actors. It can also create new actors and process its list of incoming
message in a sequential order, one at a time. Each actor is an independent
entity and they share nothing.

By definition they are scalable, because they all do the same thing without
being tied to any specific state or data. You can distribute them on one
machine, or accross several machines in a cluster.

In 1986, Erlang was officially born. It added a second principle of "let it
crash". As actors have one responsability, then do not have to bother about
handling errors. If it fails, it crashes, and the whole language embraces that.
Other actors can be added on top of the first to handle the errors, but the main
responsability of each actor does not care about errors.

More than ten years later, in 1998, Erlang was finally released as open-source.
The most interesting features of Erlang are not the language itself, but all the
primitives of it that are implemented by the Erlang VM. Today new languages are
born on top of Erlang, that uses the same VM, like Elixir (done by the core
contributors or Ruby on Rails, it is gaining some popularity lately).

Overall the talk was a nice way to start the day. I didn't feel it was closely
related to scale, but was more generic knowledge.

# Vasia Kalavri

Next talk continued on that generic knowledge pattern, with Vasia Kalavri who
told us more about graph databases. I didn't really see the link with scaling
here either, I must say.

In graph databases, relationships between objects are first class citizen, even
more than the objects themselves. You can run machine learning algorithm on
those relationships, and infer recommendation, "who to follow"-like features.
Even search in a graph database is nothing more than a shortest path algorithm.
Even the Panama Paper leaks was mapped using a graph database.

When we start talking about big data, we often think about distributed graph
processing. Vasia was here to tell us that distributed processing was not always
the right answer for graph databases. Because we are mapping relationships
between objects, the size of the dataset to map does not have a direct
correlation with the final size of the data in memory. You might think that with
your TB of user data you might need a huge cluster to compute everything.
Actually, to process its "who to follow" feature, Twitter only needs 64GB of
RAM. And that's more than 2 billion users. When you start thinking you'll need
a distributed cluster to host all your data, please think again. The chances
that you have more users than Twitter are actually quite low.

If storage space is not a relevant argument, maybe speed is? You might think
that using a cluster of several nodes will be faster than using one single node.
Actually, this is not true either. The biggest challenge in building graph
databases is cleaning the input. You will aggregate input from various sources,
in various formats, at various intervals. First thing you have to do is clean it
and process it, and this is what will take the longest. When you think of it,
your data is already kind of distributed accross all your sources. Just
process/clean it where it is before sending it to your graph database.

Then the talk went deeper into the various applications of graph databases, like
iterative value propagation, which is the algorithm used for calculating things
like Google PageRank. You basically compute a metric on each node and aggregate
it with the same metric from the neighbor nodes.

It then went into specifics of Flink, Sparks and Storm but I didn't really
manage to follow here, except that the strength of Flink was its decoupling of
streaming and batch operation while spark does streaming through micro-batches.

Once again, not really what I was expecting from dotScale here.

# Sandeepan Banerjee

After that the ex-head of data of Google, Sandeepan Banerjee, talked about
containers and how they deal with state.

His point was what we've all heard for the past two years. How Docker is
awesome, and how Docker still does not answer the question of stateful data. ow
do you handle your dependencies in a container? How do you handle logging from
a container? How do you run healthchecks on containers? Where do you save your
container database?

As long as your container only compute data coming to it, and sending it back,
you're fine. But as soon as you have to store anything anywhere, your container
starts to be linked to its surroundings. You cannot move it 

As soon as your container needs some kind of state (and most of them do), you
cannot move it easily. When an issue is raised in production, it would be nice
to be able to get the container back _with its data_ to your dev environment and
run your usual debugging tools on it, _with the production data_.It also raises
the question of integration testing of containers. How do you test that your
container is working correctly, when it is talking to other micro-services?

In a perfect world, you would be able to move a container with its data from one
cloud provider to another, as easily as if there was no data attached. You could
move it back to local dev as well and share the data with other teams so each
can debug the issue in their service, with the same data from production.

He then gave his solution, which still does not have a name, of a way to
organise containers by grouping them and moving the whole group at once. Even if
each internal container will be stateless, the group itself will have knowledge
of the associated data and relationships between containers, thus becoming
stateful.

This solution would be based on a git-like approach, where the snapshots would
not only contain relationships and data, but would also include a time
dimension. This would allow you, just like in git, to get back in time and
reload the snapshot like it was two days ago, and share a unique reference to
that moment in time with other teams. He also talked about using the same
semantics of branches and merges accross snapshots.

He went even futher talking about a GitHub like service where you could push
your container snapshots and share them, versionning your infra as you go and
integrate it into CI tools to deploy a whole network of containers with a simple
commit.

Overall this looked like a very promising idea, even if we are really far from
it. A few other talks during the day mentionned the Git approach of branches and
unique commit identifier as a way to solve a lot of issues. I hope this project
will be released one day because if it actually fixes the issues that were
raised, it will greatly help. Still, today it felt more like a wishlist than
anything real.

# Oliver Keeble

Next talk was one of the most interesting of the day, one that make you see in
perspective your own scalability issues. It was about the large hadron collider
of the CERN, and how they manage to handle all the data they got from the
experiments.

The LHC is the world's largest particle accelerator, it takes the form of a 27km
long underround tunner where the collide particles at the speed of light, and
analyse the results. It's an international scientific collaboration. When the
collision happens, things happen, things disappear, and the only things that are
left are the data the machines where able to capture.

Because such an experiment is a huge scale, they need to capture as much data as
possible per experiment. They have more than 700 tons of detection devices, that
record data at a rate of 40 millions times per seconds. That is a lot of data to
capture, send and process.

Because there is so much data, they know they won't be able to capture all of
it, so they have to make estimates based on what they captured, and extrapolate
from that. To do so, they have to calibrate the machines with empty tests, to
see the amount of data they manage to capture for a known experiment. Then, it's
just drawing a statistical picture of when they will do it with more data.

In addition to data that can failed to be recorded, data transmission can also
fail. Sending so much data from the source of the experiment to the machines
that will process it will obviously incurr some data loss in the transfer. To
counter that, they have a validation mechanisme to check that the received data
is not corrupted. It makes the transfer slower, but is needed because every
piece of information counts.

They also have a dynamic throttling of the data sent. They analyze in real time
the percentage of success of such checksum validation, and if the percentage
improves, they will push less data. If it decrease, they will push more, which
should dynamically alter the througput to be the most efficient.
As the speaker said:

> You'll always be disapointed by your in frastructure, then better to embrace
> it.

Because of its international structure, the data analysis of the data is done in
parallel in every parts of the globe, along what they call _the grid_. Machines
in all lab research facility can help computing the data. They just register to
a queue of available machine, announcing their current load. The CERN on the
other will submit jobs to another queue and attribute each job to a matching
machine. Once the job is finished on the machine, it will just call home with
the results and final results of all jobs are merged into a new queue.

It takes 25 years to build such a collider, and they are building a new one,
a 100km ring around geneva, so they expect to catch 10 times more data in the
future.

That was the kind of talk I like to see at dotScale. Real world use cases of
crazy scalability issues and how to solve them.

# Lightning talks

After the lunch break was time for the lightning talks. The overall quality was
much better than at dotSecurity.

## Datadog

First talk was by a guy from Datadog, but he never advertised its company
directly. Instead he used the same colors as the company logo, and talked about
a dating website for dogs. You know, a place to _date a dog_ *wink*. It was not
completly obvious, so fair enough :)

His talk was about the need to monitor everything, not only some key metrics,
because you never really know what metrics you should be careful about. By
logging more and more data, you can correlate parts of your infra/business
together. The more you monitor, the more you get to know your data and you can
refine non-stop what you should be looking at.

Their pricing model is based on how many metrics you keep track of, so I'm not
sure how biased his speech was, but the idea is interesting nonetheless.

## CRDTs

Then Dan Brown came again to talk about CRDT, or Conflict-free Replicated Data
Types. This gives a set of guidelines to know how to reconcile conflicts accross
two different versions of a data in an eventually consistant architecture.

By defining a standard set of rules, conflictual merges can be reasoned about
with the same outcome by everybody. When you have to changes on a boolean value,
lets use `true` by default. When changing the value of a counter, lets always
use the most positive (or negative) value. When changing the value of a simple
key/value pair, lets say that the last write wins. For arrays, if you happen to
have several conflicted add/remove jobs, give priority to the additions.

This will not avoid conflicts, but will give a commonly known set of rules to
automatically handle them, without requiring manual interaction.

## Scaling bitcoins

Then was Gilles Cadignan, about how to make a proof of existence using bitcoins.
He started with the assumption that everybody knew what bitcoin is and roughly
how it works. That was my case, I still only roughly get how it works.

In a nutshell, he explained how to embed some document hashs into the bitcoin
chain to prove that a document existed at a given date. Not sure I can give more
explanation than that, sorry.

## Monorepos and many repos

Then, Fabien Potencier from Symfony explained how the monorepo organization of
Symfony is working. They currently have 43 projects in the same monorepo. They
use this repo for development, not for deployment. They have scripts to split
thos big monorepo into individual repositories.

They still actually use both one big monorepo and several small many repos. I'm
not sure I get the story completely straight, because when I talked about it
with other attendees, we all understood something different. But what I get was
that the monorepo is the source of truth, this is where all the code related to
the Symfony project is hosted. It makes synchronizing dependencies and running
tests much easier as everything is in the same place.

But they still automatically split this main monorepo into as many repos as
there are projects. Those projects actually reference the same commit id as the
one in the main monorepo. By doing so they still keep a logical link between
commits in two repos, but they can give specific ACL to people on specific parts
of the project.

What I did not understand is how you contribute. Do you submit a PR to the main
monorepo and it will trickle down to the simple repo, or do you submit a PR to
the small repo and it will then in turn submit a PR to the main one?

# Juan Benet

After that, it was time to get back to the real talks. The next one was by Juan
Benet, about IPFS, the InterPlanetary File System. One of the most awesome and
inspiring talks of the day. To be honest, there was so much awesome content in
it that it could have lasted the whole day.

He started by reminding us how bad the whole concept of centralizing data is
bad. Distributed data is not enough, you just create local copies of your data,
but they still all depend on a central point. We have more and more connected
devices in our lives, but still, it is not possible, as of today, to easily
connect two phones together, directly. We still have to pass through external
servers to send us files. Considering the machine power of the devices we have
in our pockets, this is insane. Those devices should be able to speak to each
other without requiring a third party.

Centralization is also a security issue. Even if the connection is encrypted,
the data stored on the machine is not. It has to be stored in plain somewhere,
which means that it can be accessed, copied or altered.

Most of the time, we cannot directly access the data. We always have to use
a specific application to access its data. There is no way to directly connect
to a DB of a distant server for example, we have to go through their API (when
it exists).

Webpages are getting bigger and bigger, but we do not really care because we
have faster and faster data connection. Except when we don't. When we are in the
countryside or in the subway for example. Or leaving in a country where the data
connection infrastructure is not as good. And this is not only third world
country, this is every country in the world after it was struck by a natural
disaster such as an earthquake. And that's in that very specific moments that
you do need to have your means of communication working. But there are also the
human disasters to take into account, the government oppression, the freedom of
speech to defend...

In our current web, urls are not eternals. Links break because the destination
changed its url, because the website was redesigned, because the url shortener
was shut down or for whatever reasons.

Well, that's a huge list of problems that I can only agree with. All of this
comes from the same root cause, that all the online ressources are linked to an
address. Internet is an awesome pool of knowledge, but it is still very awkward
and old-fashion in certain aspects. 50 years ago if I were to tell you "Oh,
I read this book, it is awesome, you should read it too", you just had to go to
the closest library and borrow the book. Today with the way internet is working
you would have to go to the very specific library I'll point to you, even if the
same content could have been found somewhere else. We are not pointing at
content anymore, but at addresses to find that content.

It's a big problem, and there is a way to fix this, that draw its inspiration
from git. SVN was centralized, and it exhibited all the issues I talked about
earlier. Git and its decentralized model fixed it all.

IPFS would be like git, but for content. Each object would be linked to another
through crypto hashes, like commit hashes, and that way ensure the integrity of
the whole. You couldn't just change something in the chain of commits without
everybody knowing. And when you would talk about a specific file, through its
hash, everybody would know what you are talking about. Because it would be
distributed, there would not be any central source to get data from, you could
get it directly from any of your peers that have it as well.

There is already a lot of P2P systems that works through this logic, like
bitcoins. OpenBazaar is an eBay-like, distributed and using bitcoins as
a currency. The beauty of such a system is that you can still add some central
nodes, like GitHub does for git projects, but they won't become SPOF, they are
just here for performance and convenience.

`libp2p` is a library that integrates a lot of existing P2P protocols to build
this IPFS. All of them are compatible and allow sharing of documents through
a forest of Merkle trees from various compatibles origins.

This idea of using a git structure for various usages was discussed by a few
speakers, but this presentation really went deeper into it. Two years ago
everybody was talking about Docker in their presentations like it will be the
next big thing, I really hope that IPFS will be a reality in two years. This
will just be one of the greatest improvement in the worlds of data sharing,
hosting and security.

Such networks are already in place for some blogs, web apps and scientific
papers. Following the talk of Diogo Monica at dotSecurity, such a system will
also be of great value for package managers.

Such an awesome talk really, this is my personal highlight of the whole day.

# Greg Lindhal

Next talk was also one of my highlights. It was from Greg Lindhal, and explained
how the Internet Archive works, from the inside.

The Internet Archive is a non profit library that aim to store everything that
is produced on the internet. It contains more than 2 million videos, books and
audio recording, more than 3 million hours of television and 478 **billions** of
website captures. Overall, this weight around 25 petabytes of data.

One of their main focus is on not losing any data, ever. They store the data on
two geographic locations, and store it on two different physical disks on thoses
locations. They extract metadata from the documents and have a centralized
server that contains only the metadata and information about where to find the
complete file.

Metadata information is also stored alongside the data itself, so if the
centralized server gets corrupted, its content can still be reconstructed from
the raw data.

They get their initial data from various crawlers, some are their owns, other
are from external sources, either professional (like Alexa) or volunteers.

They have a lot of what they call _derived data_. From a given source, they can
run OCR, extract the transcript, or other kind of data. To do so, they need to
operate on a copy of the original data, to avoid corrupting the data in case
something goes wrong.

They currently have issues with their search, which is not very efficient. They
do manually build an index of all their ressources in a text file of 60TB
__compressed__. It is so big they have to build an index of the index itself.

In the future they would like to do a partnership with major browsers, that
could provide a fallback on a previous version of a website, using the Internet
Archive, in case of a 404 error. But before being able to provide that, they
need to figure out how to scale the lookup time.

They are currently operating on the maximum capacity. They plan to move on SSD,
but this is still too expensive. They are also investigating migrating to
a NoSQL database to replace their manual text index file. (it currently takes
100 days, just to index content). But this is an old
project, with hacks built on top of hacks built on top of hacks. Changing the
stack means backporting all the (undocumented) hacks. Technical debt is not
about technology, it's about people, and how they used the technology along the
years.

As the speaker was saying, all of their projects, even the smallest one can be
considered _big data_. They tried to move to ElasticSearch, but 100GB of their
data exponentially explose to 12TB once put into ES.

Suprisingly, they do not receive that many take down requests for copyright
infringement, but this is mainly because their search is so bad, that it is
nearly impossible to find something. I especially like this quote by the speaker
about that:

> First thing you do when building a website search engine with ElasticSearch is
> to replace the default ES algorithm.

# Scott

Then it was a talk from Scott, from Cloudera. He mainly talked about machine
learning in the context of content recommandation for music genres. It went
quickly too technical for me to follow the main idea.

He told that what started as a machine learning project, quickly became a big
data project. The realized that to build a relevant content recommendation
system, they will need data from their users. They uses Spark to handle the
processing of the data, because the technology is apparently well suited for
scaling this kind of recommendation and machine learning issue.

As I said, he quickly started giving a mathematical explanation of how to
compute two matrix of music genres, with specific tips and tricks to make it
faster to compute. I cannot remember the exact tricks of this specific use-case,
excepte that he suggested to always use the native math libs of Sparks instead
of the JVM for anything math-heavy related.

So yeah, sorry this recap is not as thorough as the others, that's really all
I can say about it.

# Cockroack

The next talk was a joke. The speaker was supposed to tell us more about
CockroachDB, but he seemed like he had a plane to catch. He did it's 1 hour long
presentation in less than 20mn.

We barely had time to read a slide before he jumped to the next. I think the
only thing the audience grasped from his talk was that he knew what he was
talking about. Good for him, because honestly, I think he was the only one.

# Gleb Budman

Fortunatly, the next talk was its exact opposite and was one of my favorites
from the day. Gleb Budman, from BackBlaze gave an overview of how they built
BackBlaze from the first days.

BackBlaze is a cloud backup system. They offer unlimited backup for 5$/month
(unfortunatly not compatible with Linux, otherwise I would already be
a customer). What is really interesting is their approach into building such
a system, in an iterative process.

They first had to choose where to store the data. They couldn't go to Amazon
because for 5$ a month, they could only get 30GB, and they needed to offer
infinite storage space, because you never know how much data you'll have to
save. And you want a backup system that "just works", without you worrying about
disk space.

So they started to investigate into buying their own hardware. They quickly
realized that they could buy a 1TB drive for 100$ in a local shop, while it
would cost more than 1000$ for the same capacity for its server-side version.

They never imagined that they would start building their own hardware. Amazon
already do that, and there was no way they could be more cost efficient than
Amazon. But it was either trying, or closing the company. So they tried.

They started with external drives and cascading USB hubs. Didn't work.

So they started thinking about what they do **not** need. They are only storing
data, they do not need to run application on their hardware. The data will only
be accessible from time to time, not continuously. The various machines will not
need to be able to talk to each other directly.

But they do need to be cost and space efficient. They need to be able to store
as much data as possible in the smallest possible space, without being too
expensive. So they bought a bulk of hard drives. Not high quality ones, but
commodity parts. They created a custom case, made out of wood, to host as many
drives as possible.

This worked well, but didn't scale really well. They will need more and more of
those cases if they want to grow. So they decided to do some Agile, but for
hardware. They contacted a company that could build a metal case from the wood
prototype. And in small iterations of 4 weeks, they manage to learn a lot about
what they needed, by testing it constantly, and improving each case from the
knowledge they got from the previous case. They tested different shops,
different 3D printing techniques, and regularly improved the hard drive case.

But then developers started to ask for an API, to directly access the data. So
they needed to keep iterating on the case, but now to hold hardware that could
host both the data and run a webserver.

They managed to fit 60 hard drive in a box, or what they call a pod. Because
it's commodity hardware, it will fail. So they replicate the data accross the
drives. For each chunk of 20 drives, they can afford 3 of them down at any time.
The data is replicated enough so that they could rebuild the missing pieces as
long as 17 drives are working.

He then give more details about some real life issues they had when building the
pods. Like forgetting to put a hole to set a power button, or putting a coat of
paint that was too thick and prevented the case to fit in the datacenter. Or how
a flood in thailand stopped the wordwide production of hard drives and how they
had to manually go buy external drives and rip them out to get the hard drives
to put in the pods.

Overall, his talk was really inspiring. Telling us than there is nothing more
important than field knowledge. Go test your product on the field as soon as
possible. It can work with hardware as well as with software. Find ways to
build, even if not perfect, you will learn and you will improve it over time.
Toss the unnecessary, don't buy things that are too expensive and provide
features you don't need. Create prototypes fast, and often.

# Ted Dunning

Then Ted Dunning talked about real time processing, and how streams are the
future. The talk seemed really meta and high level and hard to follow.

He started by saying that everything in the world works in a symmetric way, with
an associated conservative law, like energy and time. And that this is true both
in the microscopic and macroscopic world. He then throw a few dollar bills on
the floor, telling us that if we had to pick those bills, the most clever way
would be to pick the highest value bills first. I told you it was meta.

The idea behind the bills show is that, if bytes were money, we should pick the
best bytes first. A data processing system should first take the more
interesting data, and then continue by picking the less interesting data. This
would create a graph were the value we get from the data is high at first, and
then just hit a plateau where we got less and less value per data. The flatter
the graph goes, the more expensive it gets to get value.

He then went on talking about the fact that data should never be altered. That
data should only be reasoned about in terms of now, after and before. That all
processing of data is actually a stream, and that data will just keep growing
and that scaling is inevitable.

He also went on talking about the communication cost of developers. In a perfect
world, putting more developer on a task should make this task faster to resolve.
In practice, this adds a communication cost. There is an optimum number of
developers for a given task where the boost in productivity outweight the cost
of the communication. Each company, and even each project, should find this
optimum.

He concluded by saying that even if REST is nice, and that you can technically
do anything with it, it still creates a communication cost in between all the
nodes that needs to communicate. On the other hand, streaming (through something
like Kafka), removes the need for coordination, or acknowledgement, and will win
this part in the future.

To be honest, I had a hard time following where he wanted to bring us with this
talk. I am still not sure what is point was, as you may have noticed from my
notes.

# Eliot Horowitz

Co-founder, CTO of MongoDB

microserviveces, third parties
how do you keep data consustent accros all thos eservices. not gonna talk about
it

How to look at the dat. Each one has itsown way to see its own data. Can't see
everuhing together
lots of data, analyrics salesfors, jira, NPS, mailing, etc

you can build an agregate of the date, like a report, but it can take time and
money and more importantly, you cannot search in it. ou might not know what
you're looking for

data warehouse, whereyou put structred data in it, and you search in it
(but pr€-processed data, new data coming and new schemas that do not fit with
the previous one)

data lake, you put everything in it, with various schemas, and this is not
longer a data ake but a data dump
no way to structr this kind of data, became a drowned warehouse

start again
le'ts not assume iu have to put the data in the same place
we eneed flexibility, easy expliration, help BI, and nice to be able to ingest
any data easuly without forced by a stuctred schema

(create chorts. group of users
"are user that read the production note have a better NPS?"
see if that, better score of something else

keep the data where it is
build a service that can query all services
mongo aggregate to get daa from various sources, pipeline to add data from other
sources, filter on some behavior, and group them by behavior. calculate an
average of a metric on those users
(fetch data from third parties?)

everyt query meas querying a lot of external call
because in a DB that understand the third parti sources, can optimize the
queries
can also cache some data locally
(still need to handle the error of third parties)
can set settins on some max dte for the cache, so no need to rechech it because
still in the cache. Example, SF does not serve an history, but can keep te
vamiue in the histroy of mongoDB

is that a DB or is that a frewmarok to wrte more pde o,sode yje DN
same question for ES
starts clean, butadds more power, then move things back again historically
if only data, better for optimization, but can need more information for
optimization that came from custom inside code

# Eric Brewer

VP of Infrastructure at Google

lets focus on the core value, not on containers, cloud, ops, scaling, pay for
what yyou use
forget the machines, think in terms of services

pods that contain containers that can share volumes, because all containers are
colocated
in a given pod, upi ca, share the ip address and ports

build services on top of pods
a pod is a template, ui create several pods from this temlatre
put a load balancer in front of it
all pods are the same, they are mmuatable and interchangeabe
I can restart them easily
abstract name on top of it, for the service, not fr the pod

how to deploy
1/ scripting
easy, well simple
not declarative enough, juts does things but have no ideas itself
2/ DSL
less verbose, follows a model
not enough resource types for your custom project
just a toy language not the whole tooling
interpereted in production, can introduce runtime errors

immtale is robus
containers are immutable, a set of containers a pod sjould also be immutable
graph made of immutable pods should also be immutable, so you can deploy it
again, exactly the same
(where is the data?)

construction time to build the graph and then only deploy it
if wrong, just d'ont deploy it
code should build the immutable graph, not when deploying it
you can use hgh level language to build it, more tooling

wja not to put in the container: keys
no credentials in containers. juts put them in a volume, and accesss the volum
from the conrainers
put values ito the pod, and can change them when you want iwthout rrestaring the
pod

build the inary, package with other dependencies
construct the topolgy immutable, and the deploys
4 phases, each one removes a degree f freedom (but also of loose pieces)

Google developing this in Helm /kubernetes/helm
example of rolling update in Kubernetes
adding new config with a new version of a given app, dynamically decrese the
number of pods running the previous version and increase teh number with the new
version

in the future, no more hard drive on laption everythin will be sotored online,
so drives will be for datacenters, for rethink the way drives are down
drives do not need to be more reliable to the datacenter they are in, netter to
build worse drives, cheaper, that takes less spaces, that are as reliable than
the datacenterS


new conference dotAI, day after dotScale/dotSecurity, will be too hard











eotFail? dotTest? dotData? dotDB?
Ask Sylvain about Meetup partner
