---
layout: post
title: "dotScale 2016"
tags: dotscale
---

After dotSecurity last Friday, it is now dotScale. This follow the same pattern
as the dotCSS/dotJS conferences. One short (one afternoon) on the Friday and one
longer (one full day) on the next Monday.

As usual, this took place in the Théatre de Paris. The main issue of the last
dotJS that took place here (the main room getting hotter and hotter along the
day) was fixed, which let us completely enjoy the talks. There were more than
800 attendees this time.

# Mickael Rémond

First talk was by Mickael Rémond, about the Erlang language. It was a high level
talk, explaining the history of the language. 

It started in 1973, in big telecommunication firms. It follows what is called
the Actor Model. Each actor has a single responsability and can send messages to
other actors. It can also create new actors and process its list of incoming
message in a sequential order, one at a time. Each actor is an independent
entity and they share nothing.

By definition they are scalable, because they all do the same thing without
being tied to any specific state or data. You can distribute them on one
machine, or accross several machines in a cluster.

In 1986, Erlang was officially born. It added a second principle of "let it
crash". As actors have one responsability, then do not have to bother about
handling errors. If it fails, it crashes, and the whole language embraces that.
Other actors can be added on top of the first to handle the errors, but the main
responsability of each actor does not care about errors.

More than ten years later, in 1998, Erlang was finally released as open-source.
The most interesting features of Erlang are not the language itself, but all the
primitives of it that are implemented by the Erlang VM. Today new languages are
born on top of Erlang, that uses the same VM, like Elixir (done by the core
contributors or Ruby on Rails, it is gaining some popularity lately).

Overall the talk was a nice way to start the day. I didn't feel it was closely
related to scale, but was more generic knowledge.

# Vasia Kalavri

Next talk continued on that generic knowledge pattern, with Vasia Kalavri who
told us more about graph databases. I didn't really see the link with scaling
here either, I must say.

In graph databases, relationships between objects are first class citizen, even
more than the objects themselves. You can run machine learning algorithm on
those relationships, and infer recommendation, "who to follow"-like features.
Even search in a graph database is nothing more than a shortest path algorithm.
Even the Panama Paper leaks was mapped using a graph database.

When we start talking about big data, we often think about distributed graph
processing. Vasia was here to tell us that distributed processing was not always
the right answer for graph databases. Because we are mapping relationships
between objects, the size of the dataset to map does not have a direct
correlation with the final size of the data in memory. You might think that with
your TB of user data you might need a huge cluster to compute everything.
Actually, to process its "who to follow" feature, Twitter only needs 64GB of
RAM. And that's more than 2 billion users. When you start thinking you'll need
a distributed cluster to host all your data, please think again. The chances
that you have more users than Twitter are actually quite low.

If storage space is not a relevant argument, maybe speed is? You might think
that using a cluster of several nodes will be faster than using one single node.
Actually, this is not true either. The biggest challenge in building graph
databases is cleaning the input. You will aggregate input from various sources,
in various formats, at various intervals. First thing you have to do is clean it
and process it, and this is what will take the longest. When you think of it,
your data is already kind of distributed accross all your sources. Just
process/clean it where it is before sending it to your graph database.

Then the talk went deeper into the various applications of graph databases, like
iterative value propagation, which is the algorithm used for calculating things
like Google PageRank. You basically compute a metric on each node and aggregate
it with the same metric from the neighbor nodes.

It then went into specifics of Flink, Sparks and Storm but I didn't really
manage to follow here, except that the strength of Flink was its decoupling of
streaming and batch operation while spark does streaming through micro-batches.

Once again, not really what I was expecting from dotScale here.

# Sandeepan Banerjee

After that the ex-head of data of Google, Sandeepan Banerjee, talked about
containers and how they deal with state.

His point was what we've all heard for the past two years. How Docker is
awesome, and how Docker still does not answer the question of stateful data. ow
do you handle your dependencies in a container? How do you handle logging from
a container? How do you run healthchecks on containers? Where do you save your
container database?

As long as your container only compute data coming to it, and sending it back,
you're fine. But as soon as you have to store anything anywhere, your container
starts to be linked to its surroundings. You cannot move it 

As soon as your container needs some kind of state (and most of them do), you
cannot move it easily. When an issue is raised in production, it would be nice
to be able to get the container back _with its data_ to your dev environment and
run your usual debugging tools on it, _with the production data_.It also raises
the question of integration testing of containers. How do you test that your
container is working correctly, when it is talking to other micro-services?

In a perfect world, you would be able to move a container with its data from one
cloud provider to another, as easily as if there was no data attached. You could
move it back to local dev as well and share the data with other teams so each
can debug the issue in their service, with the same data from production.

He then gave his solution, which still does not have a name, of a way to
organise containers by grouping them and moving the whole group at once. Even if
each internal container will be stateless, the group itself will have knowledge
of the associated data and relationships between containers, thus becoming
stateful.

This solution would be based on a git-like approach, where the snapshots would
not only contain relationships and data, but would also include a time
dimension. This would allow you, just like in git, to get back in time and
reload the snapshot like it was two days ago, and share a unique reference to
that moment in time with other teams. He also talked about using the same
semantics of branches and merges accross snapshots.

He went even futher talking about a GitHub like service where you could push
your container snapshots and share them, versionning your infra as you go and
integrate it into CI tools to deploy a whole network of containers with a simple
commit.

Overall this looked like a very promising idea, even if we are really far from
it. A few other talks during the day mentionned the Git approach of branches and
unique commit identifier as a way to solve a lot of issues. I hope this project
will be released one day because if it actually fixes the issues that were
raised, it will greatly help. Still, today it felt more like a wishlist than
anything real.

# Oliver Keeble

CERN
computing for he large hdyron collider
300.000 x 1To
lots of CPU, wires
dataceneters, distributed
distributed computer dream

why so much computing?
tunnel underground, 27km long
collide particles at the speed of light
international collaboration
once happens, disappeard, what is left is the data
700 ons of detector (40 millions times / s), 4 detectors
lots of data to send
data problem
lots of time spending ata from detector to the computer

does test with none data, see if can get it correctlt
then statistical picture;. more data and more data

`-K`, check value, retry, validation,because can lose data

optimiser of success
if less success, pushes less data, until good again and push mre
manage to get best throughput dynamically 
> You'll always be disapointed by your in frastructure, then embrace it

when operating at capacity, no need to imporve speed, you use everything
optimise througput, not speed

150 sites, 150 batch systems
how to chose one, how long it taes, who is available
no idea
users submit jobs to single queue. other queue is submitting ressources, that
call home with the given tasks
make no assumption to infra, juste merge two queus into one, send back data to
ressource

10x more data in te future
more complex
25 years to build a collider (100km ring geneva)

# Lightning talks

need data before improving anything
data when starting, knowing what users are doing
growing, need to know when something is wrong, based on pririty
keep monitoring everythong, gets sharper everytime, never stop

## Dan Brown

distried gices more flexibility, improe UX because close to user
hard to do between continents
can split data, but need to reflect conflict with either last time value, or
custom conflict function
can be writtet by dveelopers, but is hard an will cojfuse them
CRDTs are re-usable data types for consistency of eventula consistent data
Register, last one win logic. Onlye one key/value pair
same for Flags, true or false, only cange one line. default to true in case of
flags
a
counters
keep the most positive or most negarive value

sets are arays
add/remove, add wins
concurrent add/remove is add winning

maps... can have everything
Convergent Replicated Data Types
resolve to single correct value, given set of rules

## Gilles Cadignan

Scaling bitcoin with mrkel trees
assumption, you know what is si
and rouly know how it works

btcoin is max 3 transaction per secont

btcoin is not only money transfer, tip of the icebergs
"oriif of exusranec", secure way to timestamp documents in te bloclwhin

build a bitcoin transactions with thos document hahsh

chainpoint merkle ree several documents
can store several documents in a ytansaction
solves the problem, of proof of existence with bitcoin

## Fabien Potencier

Monorepos / Manyrepos

monorep, 43 projects, 25 000 commits
monorepo is for developemt, not deployment
easy for synchronisation of various projects

manyrepos
ounadaries betwee, rpoiejcts, reussale, CI easier, access control

manyrepo are extracted from the monorepo
mono is source of truth, real time sync to many repos, but do PR on the mono
`pslit.sh` does in in ms, instead of minutes from the git split official command


# Juan Benet
IPFS

Happy to be here
computation is function, whcii is changing data

Intrerplanateor File System

centralize data is bad, sick
can be distribted, but is stll a sngle point
everytime something computed must be recompited
whe have more and more iot, stop talking to centralize server
today even with mobile supercompute, still need to move files through online
server

secuirty issue, tls, biy not encryptuon at rest
should do better today
can't access data directly, instead must access the aplicatio that got access to
the data
we still have latency and bandwith issues, and the pages are getting biger and
bigeger, and we are blocking access to bad connection
not only on third world country, can also be wjen we have disasters
human  distasters, surprise oppression, freedom of speech

likes break and documents can be no longer here even if we have the link
or change url of documents at the source

lots of probemes
issue from location model
address = ip
"got to that libraray to find this book", while books should be found everywhere
even if several sources for the same ocntent, you can only access the one at the
end of the dns

big big problem

git is decentralize, svn was centralized
disributed, everbody is client/server

IPFS is git for al content
one object is linked to another through crypto hahs
you can have it from anywahre, and chekc integrity

Conflict-free replicated Data types
always esolve conflic the same way

lots of P2P 2.0 systemes, like bitcoins
Ethereum, OpenBazaar (eBay distributed, using bitcoin)
central points improve perf, but not as a SPOF

markle tree prove data, cannot be altered without others knowing

will be faster, decentralize, links will not die, secured, can't be altered

libp2p, collection of perr to peer protocolas
nice interfaces to get it through all protoclos, through their hashes
"forest" of merkle tree accross several protocols

can apply git to anything, fileststem, network
expose data directly, expose it, link it everywhere

network alreayd in place, blog, scteific papers, web apps
haadcode/orbit
decentralize package manager, secured liks
use for achive of wikipedia, everything

think of the software implication
people will depend on it, and on its failure mode
give superpower, deal with this social contfact
don't break it, great power great responsabilities

AWESOME TALK

# Greg Lindhal

Internet archive, non profil library
archive 90.000 software tiels, can even run them in the browser
2.000.000 videos
2.300.000 scanned books
2.400.000 audio recordings
3.000.000 hours of television
4.000.000 eBooks
478.000.000.000 web captures of sites

25PB


do well: do not loose data
two geographicl copies of data, and written on eac location on two disks
lots of metadata. centralized metadata server to tell you where to get it
can be recovered by looking at content of all disks
storen metadata alon ith data
derived files, transcoding, ocr, epub

difefrent crawlers, official, elxa, volunteers


very careful with the data, few compiye resources
decicated to derive, only access copy of data, do not mess it up

isseu, slow to search, don't know how to store results
just storing basically

index is a big file (60TB compressed)
has an index of index

wat to offer in browser when 404 using the wayback machine
need to scale the lookup time

real on the edge of infra, issue will affects playback of
will move to SSD. looking for NoSQL of some sort instead of manual index file

don't work like google with time machine
iterate over html, extract links, put them into elastic search
100 days just to index

can't to that for derived, can't copy the data t operate on it
non-profit, hard to build stuff without moneyr
awesome project
works, but n =o idea how to scale it

all their projects are "big data"
100Gb of their data, moved to ES became 12TB

20 yo organization hard to change things
better to change the system with another
but 20 years of hacks in it
not a startup where you can change the stack easily

technical debt is about people, and how they used the tech during the years

get some donation, founder of alexa
also do book scanning for 400+ parties
15 milion/year

no takedown resuqest, because hard to find stuff on the archive

> first thing you do when building a site search enging, first thing you do is
> replace the ES algo

a bit manual



# Scott

Cloudera

Mahcine learning

content recommendantion
intuitive, familier
more date, better recommendaton
mahcine learing issue that became a big ata issue

nice use case for spark, scale recommendation and other ML problems

tastes
diffreent tastes, bnot linked together, but there is a reason
maybe under the umbrella of rock and folk
matrix of users and genres
and genre to albums

mathematcical apxlanatin, matrix
cmputation of two matrix to find user data
tips and tricks, to make it faster for compiyer
native math libs in spark instead of the jvm

spark
use emore instead of disk
find better math equality
join blocks inteligently beforehand to get it faster
use native lmath libs acceleration

#

Isoloation of transactions
RAFT, technical slides, really fast
too quick, can't even follow from one slide to the next where he is oving
cockroach

# Gleb Budman

BackBlaze
HadrWare

lost compute, no backup
build a cloud backup server
unlimited, backup everything
5$/month

where toput the data
amazon? onty 30GB on 5$
buy own servers. 1To 100$, but 100$ in a server

build it? no way to be cheaper than amazon or more resilient than datacenters
but that or go out f business
cascading USB hubs, didn'twork

Toss the unnesessary
dont build better than exising, build better for their use case
do not run anything, not par of a cloud, do not need a SLA, redundancy
+ cost efficient, dense, efficient

use commodity arts, nothing made for enterpsie, use things made in bulk with
small margin

server and desktop power suply. does the same thing, different shape

manually assembled stuff, made out of wood, done by hand
works for one, but can't scale

100$ dollar, prototype of case, form the plywood to a metal case

Agile for hardware, iterateon hardware, 4 weeks
acklog,iterate in 4 weeks
get as much done as possible, learn for it
find faseer sops, 3D printing, printing, etc

deelopers asked for direct access to the service through and API
need to continue to iterate, through teh hardware to et something that can run
a webserver

now 60HD in one box, a pod
commodity hardware, will fail
use 20 of them, store data in split in it
can still get date from 17 of them if 3 fails

ested different issues
no power button
maybe power to disk, did whole
air flow
too much paint to fit in the datacent
flood in thailand, can't buy hard drive
bought extarnal hard drive to get the inner on

toss the unneccasry
avoit IT tax
create protoypes fast
use agile for hardware

open source the pod design
wiring, cases, etc
hired engineer that way

# Ted Dunning

every example of symmery has an associae conservative law
energy and time, conservation of enery
conservation of totality done in the microscopic world
main idea of physics

if bytes were monery, we'd pick up the best bytes first
over time, grows fast first because we get the est data, and then grows slowly
on a plateau because we get less valuable bytes
less and less value from a byte on average
as value becomes flater, cost become steeper

ol dta ahsould never be changed
no flobal state, no now, only after and before
all processes are streams
and scaling everything is inevitable

building systemes in this world
in a parfect world, more deveopers shold mean faster
ut this goes with a communication cost
and if needs to speak, means that we have more coupling
there is a a specific number for each cpmpany that maximize the developer
experience wiyjut too muc time  spend on communcation. too many developers and
too many time wasted , cpùùi,ocatop,/

best go for small scale, micros sevrices
messageing systems without global transaction have "won" (?)

A talking to B, A should not have to understand how B works. B could do batch or
streaming internally, but this of no concern of A
REST is nice, scames wemm e,pihj a,d easu tp ise
not limit on REST, you can do anything
streaming is not perceived as scuh, seems to still have performance ens caling
issues. needs to be improved. Kafka is nice for this. No need for coordination
or order of acknowledgment. Thy will win this part

hard to migearate
but not too hard, there are ways
isolate part of the process, move update trugh streams bit by bit

# Eliot Horowitz

Co-founder, CTO of MongoDB

microserviveces, third parties
how do you keep data consustent accros all thos eservices. not gonna talk about
it

How to look at the dat. Each one has itsown way to see its own data. Can't see
everuhing together
lots of data, analyrics salesfors, jira, NPS, mailing, etc

you can build an agregate of the date, like a report, but it can take time and
money and more importantly, you cannot search in it. ou might not know what
you're looking for

data warehouse, whereyou put structred data in it, and you search in it
(but pr€-processed data, new data coming and new schemas that do not fit with
the previous one)

data lake, you put everything in it, with various schemas, and this is not
longer a data ake but a data dump
no way to structr this kind of data, became a drowned warehouse

start again
le'ts not assume iu have to put the data in the same place
we eneed flexibility, easy expliration, help BI, and nice to be able to ingest
any data easuly without forced by a stuctred schema

(create chorts. group of users
"are user that read the production note have a better NPS?"
see if that, better score of something else

keep the data where it is
build a service that can query all services
mongo aggregate to get daa from various sources, pipeline to add data from other
sources, filter on some behavior, and group them by behavior. calculate an
average of a metric on those users
(fetch data from third parties?)

everyt query meas querying a lot of external call
because in a DB that understand the third parti sources, can optimize the
queries
can also cache some data locally
(still need to handle the error of third parties)
can set settins on some max dte for the cache, so no need to rechech it because
still in the cache. Example, SF does not serve an history, but can keep te
vamiue in the histroy of mongoDB

is that a DB or is that a frewmarok to wrte more pde o,sode yje DN
same question for ES
starts clean, butadds more power, then move things back again historically
if only data, better for optimization, but can need more information for
optimization that came from custom inside code

# Eric Brewer

VP of Infrastructure at Google

lets focus on the core value, not on containers, cloud, ops, scaling, pay for
what yyou use
forget the machines, think in terms of services

pods that contain containers that can share volumes, because all containers are
colocated
in a given pod, upi ca, share the ip address and ports

build services on top of pods
a pod is a template, ui create several pods from this temlatre
put a load balancer in front of it
all pods are the same, they are mmuatable and interchangeabe
I can restart them easily
abstract name on top of it, for the service, not fr the pod

how to deploy
1/ scripting
easy, well simple
not declarative enough, juts does things but have no ideas itself
2/ DSL
less verbose, follows a model
not enough resource types for your custom project
just a toy language not the whole tooling
interpereted in production, can introduce runtime errors

immtale is robus
containers are immutable, a set of containers a pod sjould also be immutable
graph made of immutable pods should also be immutable, so you can deploy it
again, exactly the same
(where is the data?)

construction time to build the graph and then only deploy it
if wrong, just d'ont deploy it
code should build the immutable graph, not when deploying it
you can use hgh level language to build it, more tooling

wja not to put in the container: keys
no credentials in containers. juts put them in a volume, and accesss the volum
from the conrainers
put values ito the pod, and can change them when you want iwthout rrestaring the
pod

build the inary, package with other dependencies
construct the topolgy immutable, and the deploys
4 phases, each one removes a degree f freedom (but also of loose pieces)

Google developing this in Helm /kubernetes/helm
example of rolling update in Kubernetes
adding new config with a new version of a given app, dynamically decrese the
number of pods running the previous version and increase teh number with the new
version

in the future, no more hard drive on laption everythin will be sotored online,
so drives will be for datacenters, for rethink the way drives are down
drives do not need to be more reliable to the datacenter they are in, netter to
build worse drives, cheaper, that takes less spaces, that are as reliable than
the datacenterS


new conference dotAI, day after dotScale/dotSecurity, will be too hard











eotFail? dotTest? dotData? dotDB?
Ask Sylvain about Meetup partner
